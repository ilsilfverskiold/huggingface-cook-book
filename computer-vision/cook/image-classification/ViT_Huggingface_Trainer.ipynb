{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilsilfverskiold/smaller-models-docs/blob/main/computer-vision/cook/image-classification/ViT_Huggingface_Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyAI7kFZDKsU"
      },
      "source": [
        "**Image classification using ViT/Swin Transformer with Hugging Face Trainer**\n",
        "\n",
        "---\n",
        "\n",
        "The pre-trained model we'll fine-tune here is set for a ViT model - google/vit-base-patch16-224 - but should work well for any swin transformer model as well.\n",
        "\n",
        "Batch size is 32, epoch is 5 make sure to change these values to your preferences.\n",
        "\n",
        "**Make sure you change the dataset to what you need.** My dataset I've used has both a training and a validation set, so change the code accordingly if you don't have a validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DPlt4Uh4eWH"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"ilsilfverskiold/traffic-camera-norway-images\" # public dataset\n",
        "model_checkpoint = \"google/vit-base-patch16-224\" # decide on your pre-trained model - see the huggingface hub\n",
        "new_model_name = 'traffic-image-classification'\n",
        "learning_rate = 5e-5\n",
        "weight_decay = 0.01\n",
        "epochs = 5\n",
        "batch_size= 32"
      ],
      "metadata": {
        "id": "hdgkhI1eLjA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch dataset from huggingface or import one from somewhere else. Make sure it has been properly processed before though so the images are in PIL format.Look into the cook book for processing and pushing a custom image dataset if it's new to you."
      ],
      "metadata": {
        "id": "jAp8gdHLLz3N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwKob9DA4rtM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(dataset_url) # to fetch a private dataset use token=your_token\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look into the labels and set them so we can prepare the pre-trained model with them for fine-tuning.\n"
      ],
      "metadata": {
        "id": "Lk2wkqWKMIuF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyOdecUB4u-P"
      },
      "outputs": [],
      "source": [
        "labels = dataset[\"train\"].features[\"label\"].names\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = i\n",
        "    id2label[i] = label\n",
        "\n",
        "id2label[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the dataset for fine-tuning with ViT/ConvNEXT/Swin Transformer we'll use an image prcoessor to normalize. The image processor ensures that every input image conforms to expectations (input image size and pixel value range)."
      ],
      "metadata": {
        "id": "k665uTm0Meqx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQiqW6pU4741"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "image_processor  = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
        "image_processor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is defining a set of image transformations that are applied to the training data. These transformations prepare images for input into a neural network by normalizing them and augmenting the dataset to improve model robustness."
      ],
      "metadata": {
        "id": "lSjcYkoHMwoQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzWFM3_45QWf"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Resize,\n",
        "    Normalize,\n",
        "    CenterCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "\n",
        "train_transform = Compose([\n",
        "    Resize(256),\n",
        "    CenterCrop(224),\n",
        "    RandomHorizontalFlip(),\n",
        "    ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "val_transform = Compose([\n",
        "    Resize(256),\n",
        "    CenterCrop(224),\n",
        "    ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "def apply_transform(examples, transform):\n",
        "    examples['pixel_values'] = [transform(image.convert('RGB')) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "def set_dataset_transform(dataset, transform):\n",
        "    dataset.set_transform(lambda examples: apply_transform(examples, transform))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFwZdc845TW6"
      },
      "outputs": [],
      "source": [
        "set_dataset_transform(dataset['train'], train_transform)\n",
        "set_dataset_transform(dataset['validation'], val_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that we now have another field called pixel_values for each item below."
      ],
      "metadata": {
        "id": "tGe-3tyGMjfY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZ3e-8_n5zdy"
      },
      "outputs": [],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the labels we set up earlier from the dataset when importing the pre-trained model below, we also tell it to ignore the pre-defined labels that it previously have been trained on."
      ],
      "metadata": {
        "id": "cNkE-N1KM93q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6tfuNNR53pC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up your training metrics below.\n",
        "\n",
        "**Accuracy** indicates overall correctness, **precision** measures the reliability of positive predictions, **recall** assesses the model's ability to identify all positive samples, and **F1 score** balances precision and recall, crucial in cases of class imbalance.\n",
        "\n",
        "To understand this, if precision is relatively high, suggesting that when the model predicts an instance as positive, it is likely to be correct. However, if the recall is somewhat lower, this indicates that the model misses a significant portion of actual positive cases.\n",
        "\n",
        "To put it into perspective, for complex tasks, **especially those involving highly imbalanced datasets** or where distinguishing classes is inherently challenging, an F1 score around 0.75 - 0.80 can be considered quite ok.\n",
        "\n",
        "You'll need at least accuracy here though if you are considering to remove some of the metrics."
      ],
      "metadata": {
        "id": "Xg5ESIEuNOWB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTWet1Et6Wny"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "accuracy_metric = load_metric(\"accuracy\")\n",
        "precision_metric = load_metric(\"precision\")\n",
        "recall_metric = load_metric(\"recall\")\n",
        "f1_metric = load_metric(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels, average='macro')\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels, average='macro')\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy['accuracy'],\n",
        "        \"precision\": precision['precision'],\n",
        "        \"recall\": recall['recall'],\n",
        "        \"f1\": f1['f1']\n",
        "    }\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of the collate_fn function below is to control how a list of samples (gathered from the dataset) is merged into a single batch. This function is crucial for ensuring that batches are structured properly before being fed into a model during training or evaluation."
      ],
      "metadata": {
        "id": "my1YWJ4MPD6t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y10v11mN6bbm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up your training arguments for the Hugging Face trainer. Leave a lot of it as you can stumble onto errors if you don't. Nevertheless, you may want to play around with the learning rate, batch size and epochs used."
      ],
      "metadata": {
        "id": "ICbN--dxPlko"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_gBf07n6Dnd"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    f\"{new_model_name}\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    weight_decay=weight_decay,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B92UK9D56h8S"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model. Remember to pay attention to the training loss and validation loss, both should consistently go down and if validation keeps going up while training loss keeps going down you may be overfitting the model. Accuracy should obviously go up, and if you see very small marginal increases for every epoch then you might have reached the limit of what you can achieve.\n",
        "\n",
        "Don't worry too much if it fluctuates a bit, and try the model afterwards to see how it does as well."
      ],
      "metadata": {
        "id": "eC7kOyoaP4y-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "Jq7AwR8u6n2I",
        "outputId": "4f2da7c8-38ee-4db1-ed43-9fd47e807c33"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='235' max='235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [235/235 12:00, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.628200</td>\n",
              "      <td>0.572489</td>\n",
              "      <td>0.764359</td>\n",
              "      <td>0.793318</td>\n",
              "      <td>0.591823</td>\n",
              "      <td>0.652544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.448600</td>\n",
              "      <td>0.463022</td>\n",
              "      <td>0.801178</td>\n",
              "      <td>0.796402</td>\n",
              "      <td>0.682381</td>\n",
              "      <td>0.721348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.328500</td>\n",
              "      <td>0.439386</td>\n",
              "      <td>0.829161</td>\n",
              "      <td>0.823206</td>\n",
              "      <td>0.736606</td>\n",
              "      <td>0.772126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.181400</td>\n",
              "      <td>0.436493</td>\n",
              "      <td>0.821797</td>\n",
              "      <td>0.799302</td>\n",
              "      <td>0.736248</td>\n",
              "      <td>0.763145</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =       4.9215\n",
            "  total_flos               = 2168315527GF\n",
            "  train_loss               =       0.4179\n",
            "  train_runtime            =   0:12:04.01\n",
            "  train_samples_per_second =       42.147\n",
            "  train_steps_per_second   =        0.325\n"
          ]
        }
      ],
      "source": [
        "train_results = trainer.train()\n",
        "\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "7sRyD7RZ6paL",
        "outputId": "77b08ae9-e62d-476b-8c6a-a016836e7c91"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [22/22 00:08]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =     4.9215\n",
            "  eval_accuracy           =     0.8292\n",
            "  eval_f1                 =     0.7721\n",
            "  eval_loss               =     0.4394\n",
            "  eval_precision          =     0.8232\n",
            "  eval_recall             =     0.7366\n",
            "  eval_runtime            = 0:00:09.02\n",
            "  eval_samples_per_second =     75.256\n",
            "  eval_steps_per_second   =      2.438\n"
          ]
        }
      ],
      "source": [
        "metrics = trainer.evaluate()\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now save the model so we can do some inference on it."
      ],
      "metadata": {
        "id": "EILO10w0QvNu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STzuqvBg9Iev"
      },
      "outputs": [],
      "source": [
        "trainer.save_model('new_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xF1T3MGY9OD_"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline('image-classification', model='new_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm importing images from my Google Drive to test against. This is completely optional."
      ],
      "metadata": {
        "id": "qazXj7cWQ0OT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1u697pU9UO9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCLPuKyJ9V4y"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "image_path = '/content/drive/MyDrive/image_to_test.jpg' # path to your image\n",
        "\n",
        "image = Image.open(image_path)\n",
        "\n",
        "results = pipe(image)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) I will also run it against a few images in the validation set to see what the results are."
      ],
      "metadata": {
        "id": "vzBkJ9jaRCK_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nCJtCWv-Ye9"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "for i in range(200):\n",
        "    image_data = dataset['validation'][i]['image']\n",
        "    label_index = dataset['validation'][i]['label']\n",
        "\n",
        "    if not isinstance(image_data, Image.Image):\n",
        "        image = Image.open(image_data)\n",
        "    else:\n",
        "        image = image_data\n",
        "\n",
        "    results = pipe(image)\n",
        "\n",
        "    print(f\"Results for image {i+1}:\")\n",
        "    print(results)\n",
        "    print(\"Actual label:\", id2label[label_index])\n",
        "    print(\"----------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're satisfied, log into HuggingFace with a token that you can get via Settings in your Hugging Face account. Remember that it needs both read and write access. It will ask you for this token below."
      ],
      "metadata": {
        "id": "n8pC9wp_RO1c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7adxTewDw3U"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kni2ymLTD02x"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(new_model_name) # set Private=True if you want the model as private"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyN6b+ufdxufGP9k37hCVpFr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}