{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo0Tw4tgdaMwOpOAzL+CiP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilsilfverskiold/smaller-models-docs/blob/main/computer-vision/cook/image-classification/dataset/Image_dataset_push_huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Push Custom Image Dataset to HuggingFace**\n",
        "\n",
        "---\n",
        "\n",
        "This dataset has been stored in Google Drive, and has images seperated by folders which will be interpreted as your labels. So, if I have a dataset with traffic images, I will put the high traffic images into a folder called high-traffic, my low traffic images into a folder called low-traffic and so on. The folders themselves will become your labels when you push the dataset to the huggingface library.\n",
        "\n",
        "It will check for files that are not images (this is optional) to make sure you don't stumble onto issues later when you're fine tuning with the dataset.\n",
        "\n",
        "You'll need a huggingface account and a token. Find your token under Settings, and make sure it has read/write access."
      ],
      "metadata": {
        "id": "fGMaJpZ5YIov"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUgwDV6EhHqb"
      },
      "outputs": [],
      "source": [
        "!pip install datasets numpy huggingface_hub --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KGGDIYZLhPJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for corrupt images in the folder (this is optional)"
      ],
      "metadata": {
        "id": "idnLUHN8Y6lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/your-image-folder' #remember to change this to where you have your images located.\n",
        "\n",
        "def verify_images(folder_path):\n",
        "    for subdir, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            filepath = os.path.join(subdir, file)\n",
        "            try:\n",
        "                with Image.open(filepath) as img:\n",
        "                    img.verify()\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(f'Corrupt image: {filepath} | Error: {e}')\n",
        "                os.remove(filepath)\n",
        "\n",
        "verify_images(dataset_path)"
      ],
      "metadata": {
        "id": "oFT4nTdQhPnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "yQoVVMzWRIXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we load the dataset (that has been checked) and check the features of it. Now we'll see the labels that have been set based on our file structure."
      ],
      "metadata": {
        "id": "l1NyCJUoZdwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('imagefolder', data_dir=dataset_path)\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "mQO_lwVVCHTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'].features)\n",
        "print(dataset['train'].features['label'].names)"
      ],
      "metadata": {
        "id": "PG0hB9lMhSZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into a train and validation set. This is also optional."
      ],
      "metadata": {
        "id": "-6UfTvLnZqW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "\n",
        "split_datasets = train_dataset.train_test_split(test_size=0.1, seed=42, stratify_by_column='label')\n",
        "\n",
        "train_dataset = split_datasets['train']\n",
        "val_dataset = split_datasets['test']\n",
        "\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': val_dataset\n",
        "})"
      ],
      "metadata": {
        "id": "nID-NLBmhWS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Login to huggingface to upload the dataset. Remember you'll need your token here."
      ],
      "metadata": {
        "id": "PlhZyiWiZz6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hr8P-BehYAe",
        "outputId": "82d11cef-7713-43dd-9447-23cb17d8360d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) \n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_name = \"ilsilfverskiold/traffic-camera-norway-images\" # remember to change this\n",
        "dataset_dict.push_to_hub(repo_name)"
      ],
      "metadata": {
        "id": "LkYykytlhYti"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}