{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPjZP9Wr8PV9RTl4DaIKQbh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilsilfverskiold/smaller-models-docs/blob/main/computer-vision/cook/image-classification/ViT_Huggingface_Custom_Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image classification using ViT/Swin Transformer with a Custom Hugging Face Trainer**\n",
        "\n",
        "---\n",
        "\n",
        "The pre-trained model we'll fine-tune here is set for a ViT model - google/vit-base-patch16-224 - but should work well for any SWIN Transformer/ConvNEXT model as well. I've customized the trainer here for weight imbalances between the classes. Use it as a starting point.\n",
        "\n",
        "Batch size is 32, epoch is 5 make sure to change these values to your preferences.\n",
        "\n",
        "**Make sure you change the dataset to what you need.** My dataset I've used has both a training and a validation set, so change the code accordingly if you don't have a validation set."
      ],
      "metadata": {
        "id": "upJGV2PUSilC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DPlt4Uh4eWH"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"ilsilfverskiold/traffic-camera-norway-images\" # public dataset\n",
        "model_checkpoint = \"google/vit-base-patch16-224\" # decide on your pre-trained model - see the huggingface hub\n",
        "new_model_name = 'traffic-image-classification'\n",
        "learning_rate = 5e-5\n",
        "weight_decay = 0.01\n",
        "epochs = 5\n",
        "batch_size= 32"
      ],
      "metadata": {
        "id": "IBtz0yZqSuuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch dataset from huggingface or import one from somewhere else. Make sure it has been properly processed before though so the images are in PIL format.Look into the cook book for processing and pushing a custom image dataset if it's new to you."
      ],
      "metadata": {
        "id": "VJDP5s_MS4_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(dataset_url) # to fetch a private dataset use token=your_token\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "VwKob9DA4rtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look into the labels and set them so we can prepare the pre-trained model with them for fine-tuning."
      ],
      "metadata": {
        "id": "LSUMpGEbS82y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = dataset[\"train\"].features[\"label\"].names\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = i\n",
        "    id2label[i] = label\n",
        "\n",
        "id2label[2]"
      ],
      "metadata": {
        "id": "SyOdecUB4u-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the dataset for fine-tuning with ViT/ConvNEXT/Swin Transformer we'll use an image prcoessor to normalize. The image processor ensures that every input image conforms to expectations (input image size and pixel value range)."
      ],
      "metadata": {
        "id": "kSaoNzd9TIwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "image_processor  = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
        "image_processor"
      ],
      "metadata": {
        "id": "zQiqW6pU4741"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is defining a set of image transformations that are applied to the training data. These transformations prepare images for input into a neural network by normalizing them and augmenting the dataset to improve model robustness."
      ],
      "metadata": {
        "id": "2U4_qO5FTRNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Resize,\n",
        "    Normalize,\n",
        "    CenterCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "\n",
        "train_transform = Compose([\n",
        "    Resize(256),\n",
        "    CenterCrop(224),\n",
        "    RandomHorizontalFlip(),\n",
        "    ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "val_transform = Compose([\n",
        "    Resize(256),\n",
        "    CenterCrop(224),\n",
        "    ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "def apply_transform(examples, transform):\n",
        "    examples['pixel_values'] = [transform(image.convert('RGB')) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "def set_dataset_transform(dataset, transform):\n",
        "    dataset.set_transform(lambda examples: apply_transform(examples, transform))"
      ],
      "metadata": {
        "id": "ZzWFM3_45QWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_dataset_transform(dataset['train'], train_transform)\n",
        "set_dataset_transform(dataset['validation'], val_transform)"
      ],
      "metadata": {
        "id": "OFwZdc845TW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that we now have another field called pixel_values for each item below."
      ],
      "metadata": {
        "id": "stSHjRBGTUVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "lZ3e-8_n5zdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the labels we set up earlier from the dataset when importing the pre-trained model below, we also tell it to ignore the pre-defined labels that it previously have been trained on."
      ],
      "metadata": {
        "id": "DdTqhDOLTY_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes = True,  # set to true to ignore the pre-defined labels\n",
        ")"
      ],
      "metadata": {
        "id": "h6tfuNNR53pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up a function to calculate the class weights between the classes."
      ],
      "metadata": {
        "id": "ZmhAkyABTfxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def compute_class_weights(dataset):\n",
        "    labels = np.array([example['label'] for example in dataset])\n",
        "    class_counts = np.bincount(labels, minlength=4)\n",
        "    total_samples = len(labels)\n",
        "    class_weights = total_samples / (4 * np.maximum(class_counts, np.ones_like(class_counts)))\n",
        "    return torch.tensor(class_weights, dtype=torch.float32, device=model.device)\n",
        "\n",
        "class_weights = compute_class_weights(dataset['train'])\n",
        "\n",
        "print(\"Class Weights:\", class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4TuC0HQ_TS8",
        "outputId": "4b256098-3aa8-49f1-e173-530e6cfd71c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Weights: tensor([1.2344, 0.4786, 1.2344, 3.4443])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up your training metrics below.\n",
        "\n",
        "**Accuracy** indicates overall correctness, **precision** measures the reliability of positive predictions, **recall** assesses the model's ability to identify all positive samples, and **F1 score** balances precision and recall, crucial in cases of class imbalance.\n",
        "\n",
        "To understand this, if precision is relatively high, suggesting that when the model predicts an instance as positive, it is likely to be correct. However, if the recall is somewhat lower, this indicates that the model misses a significant portion of actual positive cases.\n",
        "\n",
        "To put it into perspective, for complex tasks, **especially those involving highly imbalanced datasets** or where distinguishing classes is inherently challenging, an F1 score around 0.75 - 0.80 can be considered quite ok.\n",
        "\n",
        "You'll need at least accuracy here though if you are considering to remove some of the metrics."
      ],
      "metadata": {
        "id": "v-FJW6jXTz7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "accuracy_metric = load_metric(\"accuracy\")\n",
        "precision_metric = load_metric(\"precision\")\n",
        "recall_metric = load_metric(\"recall\")\n",
        "f1_metric = load_metric(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels, average='macro')\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels, average='macro')\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy['accuracy'],\n",
        "        \"precision\": precision['precision'],\n",
        "        \"recall\": recall['recall'],\n",
        "        \"f1\": f1['f1']\n",
        "    }\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "kTWet1Et6Wny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of the collate_fn function below is to control how a list of samples (gathered from the dataset) is merged into a single batch. This function is crucial for ensuring that batches are structured properly before being fed into a model during training or evaluation."
      ],
      "metadata": {
        "id": "vnBZSicqT6SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ],
      "metadata": {
        "id": "Y10v11mN6bbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up your training arguments for the Hugging Face trainer. Leave a lot of it as you can stumble onto errors if you don't. Nevertheless, you may want to play around with the learning rate, batch size and epochs used (set them at the start of this notebook)"
      ],
      "metadata": {
        "id": "sgeSDV6QUZT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    f\"{new_model_name}\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=False,\n",
        ")"
      ],
      "metadata": {
        "id": "D_gBf07n6Dnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The CustomTrainer** below modifies the standard training process to include the use of weighted loss for handling class imbalance in the dataset. CustomTrainer extends the Trainer class, allowing it to utilize all the base functionalities of Trainer while overriding the standard training process by using a special approach to make sure the model pays proper attention underrepresented classes in a dataset."
      ],
      "metadata": {
        "id": "YHhg8QzEUqQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        device = model.device\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "        class_weights_device = class_weights.to(device)\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=class_weights_device)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "B92UK9D56h8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Train the model.** Remember to pay attention to the training loss and validation loss, both should consistently go down and if validation keeps going up while training loss keeps going down you may be overfitting the model. Accuracy should obviously go up, and if you see very small marginal increases for every epoch then you might have reached the limit of what you can achieve.\n",
        "\n",
        "Don't worry too much if it fluctuates a bit, and try the model afterwards to see how it does as well. Remember that if the dataset is not great then no process will make the model great so go back and look over the dataset if the model is behaving poorly."
      ],
      "metadata": {
        "id": "Ncv_ydm1VbsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "Jq7AwR8u6n2I",
        "outputId": "d000280d-51d7-4b3e-ce15-40e8f23ca998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [285/285 27:15, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.902100</td>\n",
              "      <td>0.771184</td>\n",
              "      <td>0.683374</td>\n",
              "      <td>0.623743</td>\n",
              "      <td>0.714295</td>\n",
              "      <td>0.649003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.395400</td>\n",
              "      <td>0.491489</td>\n",
              "      <td>0.798289</td>\n",
              "      <td>0.752837</td>\n",
              "      <td>0.797007</td>\n",
              "      <td>0.771331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.313900</td>\n",
              "      <td>0.434835</td>\n",
              "      <td>0.810513</td>\n",
              "      <td>0.764115</td>\n",
              "      <td>0.811026</td>\n",
              "      <td>0.782088</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =       4.9565\n",
            "  total_flos               = 6019499350GF\n",
            "  train_loss               =       0.5675\n",
            "  train_runtime            =   0:27:21.70\n",
            "  train_samples_per_second =       22.407\n",
            "  train_steps_per_second   =        0.174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "7sRyD7RZ6paL",
        "outputId": "bce417f1-bc5f-42cb-a47c-cd50f650b456"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [26/26 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =     4.9565\n",
            "  eval_accuracy           =     0.8105\n",
            "  eval_f1                 =     0.7821\n",
            "  eval_loss               =     0.4348\n",
            "  eval_precision          =     0.7641\n",
            "  eval_recall             =      0.811\n",
            "  eval_runtime            = 0:00:15.92\n",
            "  eval_samples_per_second =     51.369\n",
            "  eval_steps_per_second   =      1.633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now save the model so we can do some inference on it."
      ],
      "metadata": {
        "id": "rBCRVcgaVu6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('new_model')"
      ],
      "metadata": {
        "id": "STzuqvBg9Iev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline('image-classification', model='new_model')"
      ],
      "metadata": {
        "id": "xF1T3MGY9OD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm importing images from my Google Drive to test against. This is completely optional."
      ],
      "metadata": {
        "id": "NMnwvOD8Vynj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "I1u697pU9UO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "image_path = '/content/drive/MyDrive/my_image_I_want_to_test.png'\n",
        "\n",
        "image = Image.open(image_path)\n",
        "\n",
        "results = pipe(image)\n",
        "results"
      ],
      "metadata": {
        "id": "qCLPuKyJ9V4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) I will also run it against a few images in the validation set to see what the results are."
      ],
      "metadata": {
        "id": "QRs-O348WAQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "for i in range(100):\n",
        "    image_data = dataset['validation'][i]['image']\n",
        "    label_index = dataset['validation'][i]['label']\n",
        "\n",
        "    if not isinstance(image_data, Image.Image):\n",
        "        image = Image.open(image_data)\n",
        "    else:\n",
        "        image = image_data\n",
        "\n",
        "    results = pipe(image)\n",
        "\n",
        "    print(f\"Results for image {i+1}:\")\n",
        "    print(results)\n",
        "    print(\"Actual label:\", id2label[label_index])\n",
        "    print(\"----------------------------------\")"
      ],
      "metadata": {
        "id": "3nCJtCWv-Ye9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're satisfied, log into HuggingFace with a token that you can get via Settings in your Hugging Face account. Remember that it needs both read and write access. It will ask you for this token below."
      ],
      "metadata": {
        "id": "Hg07fZTtWEXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "6Y4FfYnZWE3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(new_model_name) # set Private=True if you want the model as private"
      ],
      "metadata": {
        "id": "7aQ1RW5aWHmM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}