{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpX05BREbElXulIRm0qw2h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilsilfverskiold/transformers-nlp-docs/blob/main/cook/fine-tune/fine_tune_encoder_huggingface_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSRbAShXTAvZ"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install -U huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# import dataset from hugging face (it has two fields I'm interested in, 'text' and 'keywords')\n",
        "dataset = load_dataset(\"sunhaozhepy/ag_news_keywords_embeddings\")\n",
        "# check the dataset\n",
        "dataset"
      ],
      "metadata": {
        "id": "KZAOExzPTIXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a validate set (as it is missing in this dataset)\n",
        "# standard practice seems to be around 70-80% training, 10-20% validation and 10-20% testing but this is a larger dataset so we can keep the sets smaller\n",
        "from datasets import DatasetDict\n",
        "\n",
        "# remember that the test and validation sets should be unique so we're grabbing data only from the training set to build the validation set\n",
        "shuffled_training_set = dataset['train'].shuffle(seed=42)\n",
        "validation_set = shuffled_training_set.select(range(7600))\n",
        "\n",
        "new_training_set = shuffled_training_set.select(range(7600, len(shuffled_training_set)))\n",
        "\n",
        "new_dataset = DatasetDict({\n",
        "    'train': new_training_set,\n",
        "    'validation': validation_set,\n",
        "    'test': dataset['test']  # unchanged\n",
        "})\n",
        "\n",
        "new_dataset"
      ],
      "metadata": {
        "id": "zCpTPvmRTN7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map out some examples from the dataset\n",
        "def show_samples(dataset, num_samples=3, seed=42):\n",
        "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
        "    for example in sample:\n",
        "        print(f\"\\n'>> Text: {example['text']}'\")\n",
        "        print(f\"'>> Keywords: {example['keywords']}'\")\n",
        "\n",
        "\n",
        "show_samples(new_dataset)"
      ],
      "metadata": {
        "id": "vxea1zi1TPLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (optional)\n",
        "# might be good to graph the distribution of length for your fields and filter out any outliers here\n",
        "# use matplotlib"
      ],
      "metadata": {
        "id": "7c-Va7csWLRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "model_name = 'facebook/bart-large'\n",
        "# get the tokenizer from Hugging Face based on the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "texts = new_dataset['train']['text']\n",
        "\n",
        "# tokenize all texts and find the maximum length of the texts - max token length for BART seems to be 1024 tokens\n",
        "max_token_length = max(len(tokenizer.encode(text, truncation=True)) for text in texts)\n",
        "print(f\"The longest text is {max_token_length} tokens long.\")\n",
        "# if it is longer than 1024 you'll need to filter or truncuate the texts in some way"
      ],
      "metadata": {
        "id": "3PxaQOvMTasF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (optional) create a function that filters out any rows with more than 5 keywords\n",
        "# this is merely to make sure we train it on the data and results we want (3 - 5 keywords)\n",
        "def filter_keywords(example):\n",
        "    return len(example['keywords'].split(', ')) <= 5\n",
        "\n",
        "new_dataset = new_dataset.filter(filter_keywords)\n",
        "\n",
        "new_dataset"
      ],
      "metadata": {
        "id": "SZjc1yGxT5eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a function that will convert both the input text and the target text into a format suitable for training a sequence-to-sequence model\n",
        "# remember data preprocessing functions would look different if you were using a model with a different architecture, such as an encoder-only or decoder-only model.\n",
        "def get_feature(batch):\n",
        "  encodings = tokenizer(batch['text'], text_target=batch['keywords'],\n",
        "                        max_length=1024, truncation=True)\n",
        "\n",
        "  encodings = {'input_ids': encodings['input_ids'],\n",
        "               'attention_mask': encodings['attention_mask'],\n",
        "               'labels': encodings['labels']}\n",
        "\n",
        "  return encodings\n"
      ],
      "metadata": {
        "id": "1J3_IBPcUOvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the tokens for the entire dataset using the get_feature function\n",
        "dataset_pt = new_dataset.map(get_feature, batched=True)\n",
        "\n",
        "# if we log this now it should show us a few more fields that are necessary for training the model\n",
        "dataset_pt"
      ],
      "metadata": {
        "id": "oCUu8VNYUeYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset should be formatted as PyTorch tensors with only the new fields\n",
        "# i.e. specifies which columns should be returned when accessing the data - only the new fields will be returned\n",
        "columns = ['input_ids', 'labels', 'attention_mask']\n",
        "dataset_pt.set_format(type='torch', columns=columns)\n",
        "\n",
        "dataset_pt"
      ],
      "metadata": {
        "id": "Uq4PRjv7UfLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the data collator is responsible for dynamically padding the batches to the maximum length in each batch.\n",
        "# which is crucial for efficient training of transformer models like BART or T5.\n",
        "# padding will look different depending on the type of model you use, you can see here that this one is specifically for seq-to-seq\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "pOvGtpyhUiWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# using the Trainer API which abstracts away a lot of complexity\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = 'bart_keywords',\n",
        "    num_train_epochs=3, # your choice\n",
        "    warmup_steps = 500,\n",
        "    per_device_train_batch_size=8, # keep a small batch size when working with a small GPU - if working with T4 set this to 4\n",
        "    per_device_eval_batch_size=8, # If working with T4 set this to 4\n",
        "    weight_decay = 0.01, # helps prevent overfitting\n",
        "    logging_steps = 10,\n",
        "    evaluation_strategy = 'steps',\n",
        "    eval_steps=500, # base this on the size of your dataset and number of training epochs (we're using a large dataset here)\n",
        "    save_steps=1e6,\n",
        "    gradient_accumulation_steps=16 # running this on a small GPU\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer, data_collator=data_collator,\n",
        "                  train_dataset = dataset_pt['train'], eval_dataset = dataset_pt['validation'])\n",
        "\n",
        "trainer.train() # may take 1-4 hours depending on horsepower (GPU) and size of model\n",
        "\n",
        "# note: trainer loss should go down, the validation loss may fluctuate for each evaluation step but consistently increasing validation, while training is going down could be a sign of overfitting."
      ],
      "metadata": {
        "id": "81bGOHX5UnH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "# make sure you set the name of the model you want here\n",
        "trainer.save_model('bart_keywords_model')"
      ],
      "metadata": {
        "id": "E7adhg3oVBuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model with the test set\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline('summarization', model='bart_keywords_model')\n",
        "\n",
        "text = new_dataset['test'][0]['text']\n",
        "keywords = new_dataset['test'][0]['keywords']\n",
        "\n",
        "print(text_test)\n",
        "print(pipe(text_test))\n",
        "print(keywords)\n",
        "\n",
        "# you can iterate over several examples from the test set to see how it is doing with new data"
      ],
      "metadata": {
        "id": "Jodndft1VI_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you're satisfied we can push it to Hugging Face\n",
        "# you'll need a token from your Hugging Face account to log in\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "drolK-XDVwhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you would replace your own username here\n",
        "# you do not need to create a repository beforehand\n",
        "trainer.push_to_hub(\"ilsilfverskiold/bart_keywords\")"
      ],
      "metadata": {
        "id": "eFI2V_bGV17n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
