{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSfgelz4eF9V3MM6O+4s85",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilsilfverskiold/transformers-nlp-docs/blob/main/cook/fine-tune/fine_tune_encoder_classification_custom_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tune an Encoder Model (BERT, RoBERTa) for text classification with a custom dataset\n",
        "This cook book is for text classification primarily. Be aware if you're training a BERT model for a different task (like question-answering).\n",
        "\n",
        "Text classification typically requires 'text' and a 'label'. This script will perform multi-class classification, where the model learns to predict the category of each keyword. Here each keyword (i.e. 'text') has a corresponding label (i.e. 'category').\n",
        "\n",
        "Only has an encoder part (e.g., BERT, RoBERTa). Suitable for classification, entity recognition, etc. Only tokenizes the input text. Output is often a single label or a set of labels (not tokenized).\n",
        "\n",
        "Remember when working with classification, you need an even distribution of examples for the different labels or the model will favor the ones with more examples. The more niche your labels are the more difficult it will be to train your model.\n",
        "\n",
        "Make sure you set your runtime to T4 or better before running the script and look out for overfitting."
      ],
      "metadata": {
        "id": "aEU3AuZ94P2r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvgSDh7U0Pmx"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install -U huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hYA6a5Il_CnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# import file - make sure you set the correct path in your Google Drive (this would be the file I'm importing)\n",
        "file_path = '/content/drive/My Drive/keywords_categories_even_distribution.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "U6E_Vy7n_DO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# remove any null values for the 'label' field - will cause issues later if there are any\n",
        "df = df[df['label'].notnull()]\n",
        "\n",
        "# Split dataset into training and temp (15%) (for validation and testing)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.15, random_state=42)\n",
        "\n",
        "# Split temp into validation and testing (split 30% for testing - change if needed be)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "olkU6QTM_Kdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "-OlqHf3kAr2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataset dict with the train, validate and test set\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': val_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "# print the dict\n",
        "dataset_dict"
      ],
      "metadata": {
        "id": "SZnLGNtPAuvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) map out some examples from the dataset\n",
        "def show_samples(dataset, num_samples=10, seed=42):\n",
        "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
        "    for example in sample:\n",
        "        print(f\"\\n'>> Text: {example['text']}'\")\n",
        "        print(f\"'>> Label: {example['label']}'\")\n",
        "\n",
        "\n",
        "show_samples(dataset_dict)"
      ],
      "metadata": {
        "id": "lEzmsEKjA0_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the LabelEncoder on all unique category labels in the DataFrame\n",
        "label_encoder.fit(df['label'])\n",
        "\n",
        "# Define a function to encode the categories in the dataset_dict\n",
        "def encode_labels(example):\n",
        "    # This will transform the text category to a numeric label\n",
        "    return {'encoded_label': label_encoder.transform([example['label']])[0]}\n",
        "\n",
        "# Apply the encode_labels function to each example in each split of dataset_dict\n",
        "for split in dataset_dict:\n",
        "    # The lambda function is replaced with the encode_labels function\n",
        "    dataset_dict[split] = dataset_dict[split].map(encode_labels, batched=False)\n",
        "\n",
        "# Check the number of unique labels\n",
        "num_labels = len(label_encoder.classes_)\n",
        "num_labels"
      ],
      "metadata": {
        "id": "2R98ZOTuDOaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the label encoder to a file\n",
        "joblib.dump(label_encoder, 'label_encoder.joblib')"
      ],
      "metadata": {
        "id": "wC1GbwRAIYkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = 'bert-base-uncased'  # or any other suitable encoder model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
      ],
      "metadata": {
        "id": "fzPDBaMZBxvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_data(dataset):\n",
        "    tokenized_inputs = tokenizer(dataset[\"text\"], padding=True, truncation=True, max_length=512)\n",
        "    tokenized_inputs[\"labels\"] = dataset[\"encoded_label\"]\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply this function to your dataset dictionary\n",
        "dataset_encoded = dataset_dict.map(encode_data, batched=True)\n",
        "dataset_encoded"
      ],
      "metadata": {
        "id": "WpxLd8PFCrRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ],
      "metadata": {
        "id": "st-BzCAYDZ-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer)"
      ],
      "metadata": {
        "id": "LQfStV1VDhnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='bert_classifier',\n",
        "    num_train_epochs=4,  # As needed\n",
        "    warmup_steps=100,  # As needed\n",
        "    per_device_train_batch_size=8,  # Adjust if necessary\n",
        "    per_device_eval_batch_size=16,  # Can be larger if no memory issues during eval\n",
        "    weight_decay=0.01,  # Prevent overfitting\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='steps',  # If you want more frequent feedback\n",
        "    eval_steps=100,  # Evaluate every 100 steps, adjust as needed\n",
        "    learning_rate=3e-5,  # Standard for BERT\n",
        "    save_steps=500,  # Adjust as preferred\n",
        "    gradient_accumulation_steps=4  # Increase if reducing batch size\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer, data_collator=data_collator,\n",
        "                  train_dataset = dataset_encoded['train'], eval_dataset = dataset_encoded['validation'])\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "84aNBexkD5PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "trainer.save_model('bert_classifier_model')"
      ],
      "metadata": {
        "id": "9ouddkIeFLVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import joblib\n",
        "\n",
        "# Load the label encoder\n",
        "label_encoder = joblib.load('label_encoder.joblib')\n",
        "\n",
        "# Load the pipeline for text classification with your model\n",
        "pipe = pipeline('text-classification', model='bert_classifier_model')\n",
        "\n",
        "# Loop through the test set from index 0 to 50\n",
        "for i in range(0, 50):\n",
        "    test_text = dataset_dict['test'][i]['text']\n",
        "    original_label = dataset_dict['test'][i]['label']\n",
        "\n",
        "    # Get the model's prediction\n",
        "    predicted_output = pipe(test_keyword)\n",
        "\n",
        "    # Extract the label number from the model's prediction\n",
        "    predicted_label_num = int(predicted_output[0]['label'].split('_')[-1])\n",
        "\n",
        "    # Use the LabelEncoder to get the original category name\n",
        "    predicted_label_name = label_encoder.inverse_transform([predicted_label_num])[0]\n",
        "\n",
        "    print(f\"text: {test_text}\")\n",
        "    print(f\"generated label: {predicted_label_name}\")\n",
        "    print(f\"original label: {original_label}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "_6SVH5RuGBJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you're satisfied we can push it to Hugging Face\n",
        "# You'll need a token from your Hugging Face account to log in\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "kyrdkV0p9QZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You would replace your own username here\n",
        "# You do not need to create a repository beforehand\n",
        "trainer.push_to_hub(\"huggingface_username/bert_classifier_model\")"
      ],
      "metadata": {
        "id": "IGzw5_XV9d3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the encoder too - push it or upload it manually to your model in the Hugging Face repository\n",
        "files.download('/content/label_encoder.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "k6pGv-zM90vW",
        "outputId": "3cf7bab6-65f7-4953-e8e7-b3a6d0872e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9ef28724-e8fd-4ab8-a279-18fbff87228d\", \"label_encoder.joblib\", 812)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}