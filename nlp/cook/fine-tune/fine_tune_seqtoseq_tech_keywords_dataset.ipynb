{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPk9iVkOVK2hSx4IaeC+1B3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilsilfverskiold/transformers-nlp-docs/blob/main/cook/fine-tune/fine_tune_seqtoseq_tech_keywords_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medium Tutorial - Fine-Tune a Seq2Seq Model for Keyword Extraction\n",
        "To follow the tutorial please go [here.](https://medium.com/gitconnected/fine-tune-smaller-nlp-models-with-hugging-face-for-specific-use-cases-1745813471dc). This script will create a keyword extractor using BART and the final model you'll find [here.](https://huggingface.co/ilsilfverskiold/tech-keywords-extractor). To understand what the script is doing see the [full tutorial.](https://medium.com/gitconnected/fine-tune-smaller-nlp-models-with-hugging-face-for-specific-use-cases-1745813471dc)\n",
        "\n",
        "Make sure you set your runtime to T4 or better before running the script and always look out for overfitting."
      ],
      "metadata": {
        "id": "Q9bbEQY33M23"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDhY7cBsOp76"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install -U huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# import dataset from hugging face (it has two fields I'm interested in, 'text' and 'keywords')\n",
        "dataset = load_dataset(\"ilsilfverskiold/tech-keywords-topics-summary\")\n",
        "# check the dataset\n",
        "dataset"
      ],
      "metadata": {
        "id": "l1K80KOxOsbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map out some examples from the dataset\n",
        "def show_samples(dataset, num_samples=3, seed=42):\n",
        "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
        "    for example in sample:\n",
        "        print(f\"\\n'>> Text: {example['text']}'\")\n",
        "        print(f\"'>> Keywords: {example['keywords']}'\")\n",
        "\n",
        "\n",
        "show_samples(dataset)"
      ],
      "metadata": {
        "id": "fpc-VVVmQZVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# set the correct model you'll be fine-tuning\n",
        "model_name = 'facebook/bart-large'\n",
        "# get the tokenizer for the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# check the token length of the keywords field - you can do this for both fields\n",
        "texts = dataset['train']['keywords']\n",
        "\n",
        "# Tokenize all texts and find the maximum length (max for BART is 1024 tokens)\n",
        "max_token_length = max(len(tokenizer.encode(text, truncation=True)) for text in texts)\n",
        "print(f\"The longest text is {max_token_length} tokens long.\")"
      ],
      "metadata": {
        "id": "G41UDCZiRJ3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert both the input text and the target text into a format suitable for training a sequence-to-sequence model\n",
        "# remember data preprocessing functions would look different if you were using a model with a different architecture, such as an encoder-only or decoder-only model.\n",
        "\n",
        "def get_feature(batch):\n",
        "  encodings = tokenizer(batch['text'], text_target=batch['keywords'],\n",
        "                        max_length=1024, truncation=True)\n",
        "\n",
        "  encodings = {'input_ids': encodings['input_ids'],\n",
        "               'attention_mask': encodings['attention_mask'],\n",
        "               'labels': encodings['labels']}\n",
        "\n",
        "  return encodings\n",
        "\n",
        "dataset_pt = dataset.map(get_feature, batched=True)\n",
        "dataset_pt"
      ],
      "metadata": {
        "id": "VajzCGjmShc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset should be formatted as PyTorch tensors with only the new fields\n",
        "# i.e. specifies which columns should be returned when accessing the data - only the new fields will be returned\n",
        "columns = ['input_ids', 'labels', 'attention_mask']\n",
        "dataset_pt.set_format(type='torch', columns=columns)"
      ],
      "metadata": {
        "id": "QeDRXT-ST2O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the data collator is responsible for dynamically padding the batches to the maximum length in each batch.\n",
        "# which is crucial for efficient training of transformer models like BART or T5.\n",
        "# padding will look different depending on the type of model you use, you can see here that this one is specifically for seq-to-seq\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "rUwwhB-lUGjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# start training the model\n",
        "# we're using the Trainer API which abstracts away a lot of complexity\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = 'bart_tech_keywords',\n",
        "    num_train_epochs=3, # your choice\n",
        "    warmup_steps = 500,\n",
        "    per_device_train_batch_size=4, # keep a small batch size when working with a small GPU\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay = 0.01, # helps prevent overfitting\n",
        "    logging_steps = 10,\n",
        "    evaluation_strategy = 'steps',\n",
        "    eval_steps=50, # base this on the size of your dataset and number of training epochs\n",
        "    save_steps=1e6,\n",
        "    gradient_accumulation_steps=16 # running this on a small GPU\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer, data_collator=data_collator,\n",
        "                  train_dataset = dataset_pt['train'], eval_dataset = dataset_pt['validation'])\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "YmTJMXYMV3mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "trainer.save_model('tech-keywords-extractor') # set the name you want it to be called"
      ],
      "metadata": {
        "id": "cSLEFRfsWBdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# test the model using Hugging Face's pipeline\n",
        "pipe = pipeline('summarization', model='tech-keywords-extractor')\n",
        "\n",
        "# test the first item in the test set to see how it does\n",
        "test_text=dataset_dict['test'][0]['text']\n",
        "keywords = dataset_dict['test'][0]['keywords']\n",
        "print(\"the text: \", text_test)\n",
        "print(\"generated keywords: \", pipe(test_text))\n",
        "print(\"orginal keywords : \",keywords)"
      ],
      "metadata": {
        "id": "32EBDfugWGsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over the test set to generate 50 examples at once\n",
        "for i in range(0, 50):\n",
        "    text_test = dataset_dict['test'][i]['text']\n",
        "    keywords = dataset_dict['test'][i]['keywords']\n",
        "    print(\"text: \", text_test)\n",
        "    print(\"generated keywords: \", pipe(text_test)[0]['summary_text'])\n",
        "    print(\"original keywords: \", keywords)"
      ],
      "metadata": {
        "id": "p8R3pEW7WHlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you're satisfied we can push it to Hugging Face\n",
        "# you'll need a token from your Hugging Face account to log in\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "6emFh2AqWJpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you would replace your own name here\n",
        "# you do not need to create a repository beforehand\n",
        "trainer.push_to_hub(\"ilsilfverskiold/tech-keywords-extractor\")"
      ],
      "metadata": {
        "id": "0s3F0UFBWMm_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}