{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN3C3XgYBmqGPgFOx823pPs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilsilfverskiold/smaller-models-docs/blob/main/nlp/cook/fine-tune/albert_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification with Transformers (ALBERT)\n",
        "\n",
        "This script helps you fine-tune a pre-trained model (ALBERT) and encoder model for text classification with a dataset from the HuggingFace.\n",
        "\n",
        "The use case uses binary classes to produce a model to identify clickbait versus factual content with the use of a synthetic dataset found [here](https://huggingface.co/datasets/ilsilfverskiold/clickbait_titles_synthetic_data). This script follows a tutorial that you can find here.\n",
        "\n",
        "You may use any encoder model such as BERT, RoBERTa and DeBERTa instead."
      ],
      "metadata": {
        "id": "CZrl7EHuJUfG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvzGqWjSW2sd"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install -U huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the dataset you'll be trainin on. This dataset has a 'text' field and a 'label' field. Be sure to tweak the script if you need to."
      ],
      "metadata": {
        "id": "TB6zvHZFJFMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "dataset = load_dataset(\"ilsilfverskiold/clickbait_titles_synthetic_data\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "shz4-RFOf41U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decide on your pre-trained model along with your new model's name."
      ],
      "metadata": {
        "id": "pWE97S_2I-tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"albert/albert-base-v2\"\n",
        "your_path = 'classify-clickbait'"
      ],
      "metadata": {
        "id": "GAIpslnnf6O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look over your distribution of the labels (optional)"
      ],
      "metadata": {
        "id": "5DXTBviEKBia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "train_label_distribution = Counter(dataset['train']['label'])\n",
        "test_label_distribution = Counter(dataset['test']['label'])\n",
        "\n",
        "print(\"Training Label Distribution:\", train_label_distribution)\n",
        "print(\"Test Label Distribution:\", test_label_distribution)"
      ],
      "metadata": {
        "id": "nOvRUItff915"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a label encoder that converts categorical labels to a standardized numerical format. Labels in their original categorical form (e.g., 'clickbait', 'factual') need to be converted into numerical values so that they can be processed by the algorithms."
      ],
      "metadata": {
        "id": "0ZyWkHH0KIA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "label_encoder.fit(dataset['train']['label'])\n",
        "\n",
        "def encode_labels(example):\n",
        "    return {'encoded_label': label_encoder.transform([example['label']])[0]}\n",
        "\n",
        "for split in dataset:\n",
        "    dataset[split] = dataset[split].map(encode_labels, batched=False)"
      ],
      "metadata": {
        "id": "5PeJ0mwigBQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The id2label and label2id mappings in AutoConfig are used to inform the model of the specific label-to-ID mappings so we can get the actual label names rather than the numerical reps when we do inference with the model."
      ],
      "metadata": {
        "id": "FfKX7el9LEsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "unique_labels = sorted(list(set(dataset['train']['label'])))\n",
        "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
        "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "config.id2label = id2label\n",
        "config.label2id = label2id\n",
        "\n",
        "# Verify the correct labels\n",
        "print(\"ID to Label Mapping:\", config.id2label)\n",
        "print(\"Label to ID Mapping:\", config.label2id)"
      ],
      "metadata": {
        "id": "_ECd3GJNgG-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code snippet is responsible for loading a tokenizer and a model from the Hugging Face Transformers library. Here we use ALBERT as a model, you can use AutoTokenizer and AutoModelForSequenceClassification if you want to use another model or it's specified tokenizer."
      ],
      "metadata": {
        "id": "8cTrvYVzLkoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
        "model = AlbertForSequenceClassification.from_pretrained(model_name, config=config)"
      ],
      "metadata": {
        "id": "KJoTq1K8gwGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next function makes sure the text data is properly tokenized and labeled, preparing the dataset for efficient training of the transformer model."
      ],
      "metadata": {
        "id": "dOghCGAdMdqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_invalid_content(example):\n",
        "    return isinstance(example['text'], str)\n",
        "\n",
        "dataset = dataset.filter(filter_invalid_content, batched=False)\n",
        "\n",
        "def encode_data(batch):\n",
        "    tokenized_inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=256)\n",
        "    tokenized_inputs[\"labels\"] = batch[\"encoded_label\"]\n",
        "    return tokenized_inputs\n",
        "\n",
        "dataset_encoded = dataset.map(encode_data, batched=True)\n",
        "dataset_encoded"
      ],
      "metadata": {
        "id": "N4VJZjwhg38G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ],
      "metadata": {
        "id": "iQb-rRu9g5lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DataCollatorWithPadding ensures that all input sequences in a batch are padded to the same length, using the padding logic defined by the tokenizer."
      ],
      "metadata": {
        "id": "-G5h1ET3NPLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer)"
      ],
      "metadata": {
        "id": "xg6Rb7R-g7A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll set up LabelEncoder to encode labels and defines a function to compute per-label accuracy from a confusion matrix, providing label-specific accuracy metrics. I.e. when we train the model we want to see the accuracy metrics per label as well as the average metrics. This is more relevant if you have more than two labels, and one is underperforming."
      ],
      "metadata": {
        "id": "cSctqANNNaAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(unique_labels)\n",
        "\n",
        "def per_label_accuracy(y_true, y_pred, labels):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    correct_predictions = cm.diagonal()\n",
        "    label_totals = cm.sum(axis=1)\n",
        "    per_label_acc = np.divide(correct_predictions, label_totals, out=np.zeros_like(correct_predictions, dtype=float), where=label_totals != 0)\n",
        "    return dict(zip(labels, per_label_acc))"
      ],
      "metadata": {
        "id": "zgMLYb57g-Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we set up our compute metrics. Here I've set up several, but you may reduce them if needed be. You can read more on this metrics [here.](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)"
      ],
      "metadata": {
        "id": "HTeQeL8DN05z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    decoded_labels = label_encoder.inverse_transform(labels)\n",
        "    decoded_preds = label_encoder.inverse_transform(preds)\n",
        "\n",
        "    precision = precision_score(decoded_labels, decoded_preds, average='weighted')\n",
        "    recall = recall_score(decoded_labels, decoded_preds, average='weighted')\n",
        "    f1 = f1_score(decoded_labels, decoded_preds, average='weighted')\n",
        "    acc = accuracy_score(decoded_labels, decoded_preds)\n",
        "\n",
        "    labels_list = list(label_encoder.classes_)\n",
        "    per_label_acc = per_label_accuracy(decoded_labels, decoded_preds, labels_list)\n",
        "\n",
        "    per_label_acc_metrics = {}\n",
        "    for label, accuracy in per_label_acc.items():\n",
        "        label_key = f\"accuracy_label_{label}\"\n",
        "        per_label_acc_metrics[label_key] = accuracy\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        **per_label_acc_metrics\n",
        "    }"
      ],
      "metadata": {
        "id": "HInT5WReNvTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we set up our training metrics to train the model. I'm following the paper [\"How to Fine-Tune BERT for Text Classification?\"](https://arxiv.org/abs/1905.05583) on epochs, batch size and learning rate but do play around with it if you want to.\n",
        "\n",
        "When it is in training, be sure to look out for training loss and validation loss. Both should decrease consistently. If validation is increasing consistently you may be overfitting your model and you can try to decrease number of epochs."
      ],
      "metadata": {
        "id": "Dv3scyLpORAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=your_path,\n",
        "    num_train_epochs=3,\n",
        "    warmup_steps=500,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=100,\n",
        "    learning_rate=2e-5,\n",
        "    save_steps=1000,\n",
        "    gradient_accumulation_steps=2\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_encoded['train'],\n",
        "    eval_dataset=dataset_encoded['test'],\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "yx_etZ24hBjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you're finito, you can evaluate the results, save your model and the state."
      ],
      "metadata": {
        "id": "E7L3GQsHPDRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n",
        "trainer.save_model(your_path)\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "id": "lCiYYKyqhFL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to test it out, you can run the pipeline directly with the model. I just used some new example titles to see how it did."
      ],
      "metadata": {
        "id": "Vim7CVwhPKY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline('text-classification', model='classify-clickbait')"
      ],
      "metadata": {
        "id": "pTvsu7rOhYTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_titles = [\n",
        "    \"The Controversial Truth about Tech Debt\",\n",
        "    \"A Comprehensive Guide for Getting Started with Hugging Face\",\n",
        "    \"OpenAI GPT-4o: The New Best AI Model in the World. Like in the Movies. For Free\",\n",
        "    \"GPT4 Omni — So much more than just a voice assistant\",\n",
        "    \"Building Vector Databases with FastAPI and ChromaDB\",\n",
        "    \"How Pieter Levels Makes (At Least) $210K a Month From His Laptop — With Zero Employees\",\n",
        "    \"Which Is Better: Teachers or AI in the Classroom?\",\n",
        "    \"How to Build Enterprise-Scale Generative AI Agents with AWS Bedrock: A Comprehensive Guide\",\n",
        "    \"The Best Way To Start Your One-Person Business\",\n",
        "]\n",
        "\n",
        "for title in example_titles:\n",
        "    result = pipe(title)\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"Output: {result[0]['label']}\")"
      ],
      "metadata": {
        "id": "wmG1MgK1haml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're satisfied, you can log in to HuggingFace with a token (you'll find these in your account under Settings - make sure it has write access)."
      ],
      "metadata": {
        "id": "bscp7lvwPW7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "UHx9RGTYhfqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push the model with your new name for it. It usually just takes the name you set when you trained it so whatever you put here doesn't matter."
      ],
      "metadata": {
        "id": "P98pcG_LPhoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"username/classify-clickbait\")\n",
        "trainer.push_to_hub(\"username/classify-clickbait\")"
      ],
      "metadata": {
        "id": "pVE_898bhhTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you're done. You got your text classifier."
      ],
      "metadata": {
        "id": "lUokttBHPvoc"
      }
    }
  ]
}